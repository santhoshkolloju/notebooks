{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nVGpGAmn-VwK","colab_type":"code","colab":{}},"source":["!git clone https://github.com/martiansideofthemoon/squash-generation.git\n","!cp -r squash-generation/pytorch-pretrained-BERT/pytorch_pretrained_bert .\n","!pip install -r squash-generation/requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAYshDK7-59t","colab_type":"code","outputId":"c29792a5-9558-4259-db28-d6cf419f62cf","executionInfo":{"status":"ok","timestamp":1573552060505,"user_tz":-330,"elapsed":14056,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":513}},"source":["!pip install pdfplumber"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pdfplumber\n","  Downloading https://files.pythonhosted.org/packages/ef/ef/59bb7614705d170c5a0b584beef42f3818456e788f85ca904788cd797d1f/pdfplumber-0.5.14.tar.gz\n","Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from pdfplumber) (3.0.4)\n","Collecting pycryptodome\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/60/634aa7020746e3a0793d00c6b635ae6fd19662bcb3e1e629cbe00dbbcb51/pycryptodome-3.9.2-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n","\u001b[K     |████████████████████████████████| 9.7MB 7.4MB/s \n","\u001b[?25hCollecting unicodecsv>=0.14.1\n","  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n","Collecting pdfminer.six==20181108\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 26.6MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from pdfplumber) (4.3.0)\n","Collecting wand\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/43/0fbca3a034c92f3ac989948a0920d0b4f6c0c3c3317769fcaac8dde57e35/Wand-0.5.7-py2.py3-none-any.whl (128kB)\n","\u001b[K     |████████████████████████████████| 133kB 39.1MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->pdfplumber) (2.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->pdfplumber) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=3.0.0->pdfplumber) (0.46)\n","Building wheels for collected packages: pdfplumber, unicodecsv\n","  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pdfplumber: filename=pdfplumber-0.5.14-cp36-none-any.whl size=30509 sha256=c2584b364319594fe75cd06570a322f0a5c51b04018bc45588df7def188e0271\n","  Stored in directory: /root/.cache/pip/wheels/0e/cb/1f/9287913933a24c9c67d745e91119f0c23ba239bceb6ac8fa05\n","  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp36-none-any.whl size=10767 sha256=0a8b3856f3e582529bb40dcb979385518ca5930bd3698c3eb79c415ffeb33ada\n","  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n","Successfully built pdfplumber unicodecsv\n","Installing collected packages: pycryptodome, unicodecsv, pdfminer.six, wand, pdfplumber\n","Successfully installed pdfminer.six-20181108 pdfplumber-0.5.14 pycryptodome-3.9.2 unicodecsv-0.14.1 wand-0.5.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BTni5bis_Mhw","colab_type":"code","colab":{}},"source":["import pdfplumber\n","import torch\n","import torch.nn.functional as F\n","\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from models import (OpenAIAdam, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n","                                     GPT2LMHeadModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n","import pytorch_pretrained_bert.modeling_gpt2 as gpt2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gK30OTz_aty","colab_type":"code","colab":{}},"source":["from collections import namedtuple\n","import random\n","args ={\n","    \"max_length\":45,\n","    \"min_length\":1,\n","    \"top_k\":2,\n","    \"top_p\":0.99,\n","    \"device\":\"cpu\",\n","    \"temperature\":1.5,\n","    \"no_sample\":False,\n","    \"seed\":42,\n","    \"checkpoint_name\":\"pytorch_model.bin\",\n","    \"model_folder\" :\"models/question-generation/drop_squad/\"\n","}\n","\n","SPECIAL_TOKENS = [\n","    \"<bos>\", \"<eos>\", \"<context>\",\"<question>\",\"<answer>\",\"<pad>\"\n","]\n","MODEL_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.set_special_tokens(SPECIAL_TOKENS)\n","bos, eos, context,question_id,answer_id,pad = \\\n","          tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","\n","special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","\n","\n","args = namedtuple(\"GenricDict\",args.keys())(**args)\n","random.seed(args.seed)\n","torch.random.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","\n","\n","gpt2.WEIGHTS_NAME = \"checkpoint_mymodel_250000.bin\"\n","model = GPT2LMHeadModel.from_pretrained(\"question_generation_models/drop_squad/\")\n","model.set_num_special_tokens(len(SPECIAL_TOKENS))\n","model.eval()\n","\n","def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n","        Args:\n","            logits: logits distribution shape (vocabulary size)\n","            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n","            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n","                whose total probability mass is greater than or equal to the threshold top_p.\n","                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n","                the threshold top_p.\n","            threshold: a minimal threshold to keep logits\n","    \"\"\"\n","    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n","    top_k = min(top_k, logits.size(-1))\n","    if top_k > 0:\n","        # Remove all tokens with a probability less than the last token in the top-k tokens\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        # Compute cumulative probabilities of sorted tokens\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probabilities > top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        # Back to unsorted indices and set them to -infinity\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = filter_value\n","\n","    indices_to_remove = logits < threshold\n","    logits[indices_to_remove] = filter_value\n","\n","    return logits"],"execution_count":0,"outputs":[]}]}