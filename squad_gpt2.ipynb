{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"squad_gpt2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_wFoQVbGi-uC","colab_type":"code","outputId":"5ca1b8f8-4a1f-4acf-e857-be6d95b1efd4","executionInfo":{"status":"ok","timestamp":1573453252498,"user_tz":-330,"elapsed":7770,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["!git clone https://github.com/rajpurkar/SQuAD-explorer.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'SQuAD-explorer'...\n","remote: Enumerating objects: 155, done.\u001b[K\n","remote: Counting objects:   0% (1/155)\u001b[K\rremote: Counting objects:   1% (2/155)\u001b[K\rremote: Counting objects:   2% (4/155)\u001b[K\rremote: Counting objects:   3% (5/155)\u001b[K\rremote: Counting objects:   4% (7/155)\u001b[K\rremote: Counting objects:   5% (8/155)\u001b[K\rremote: Counting objects:   6% (10/155)\u001b[K\rremote: Counting objects:   7% (11/155)\u001b[K\rremote: Counting objects:   8% (13/155)\u001b[K\rremote: Counting objects:   9% (14/155)\u001b[K\rremote: Counting objects:  10% (16/155)\u001b[K\rremote: Counting objects:  11% (18/155)\u001b[K\rremote: Counting objects:  12% (19/155)\u001b[K\rremote: Counting objects:  13% (21/155)\u001b[K\rremote: Counting objects:  14% (22/155)\u001b[K\rremote: Counting objects:  15% (24/155)\u001b[K\rremote: Counting objects:  16% (25/155)\u001b[K\rremote: Counting objects:  17% (27/155)\u001b[K\rremote: Counting objects:  18% (28/155)\u001b[K\rremote: Counting objects:  19% (30/155)\u001b[K\rremote: Counting objects:  20% (31/155)\u001b[K\rremote: Counting objects:  21% (33/155)\u001b[K\rremote: Counting objects:  22% (35/155)\u001b[K\rremote: Counting objects:  23% (36/155)\u001b[K\rremote: Counting objects:  24% (38/155)\u001b[K\rremote: Counting objects:  25% (39/155)\u001b[K\rremote: Counting objects:  26% (41/155)\u001b[K\rremote: Counting objects:  27% (42/155)\u001b[K\rremote: Counting objects:  28% (44/155)\u001b[K\rremote: Counting objects:  29% (45/155)\u001b[K\rremote: Counting objects:  30% (47/155)\u001b[K\rremote: Counting objects:  31% (49/155)\u001b[K\rremote: Counting objects:  32% (50/155)\u001b[K\rremote: Counting objects:  33% (52/155)\u001b[K\rremote: Counting objects:  34% (53/155)\u001b[K\rremote: Counting objects:  35% (55/155)\u001b[K\rremote: Counting objects:  36% (56/155)\u001b[K\rremote: Counting objects:  37% (58/155)\u001b[K\rremote: Counting objects:  38% (59/155)\u001b[K\rremote: Counting objects:  39% (61/155)\u001b[K\rremote: Counting objects:  40% (62/155)\u001b[K\rremote: Counting objects:  41% (64/155)\u001b[K\rremote: Counting objects:  42% (66/155)\u001b[K\rremote: Counting objects:  43% (67/155)\u001b[K\rremote: Counting objects:  44% (69/155)\u001b[K\rremote: Counting objects:  45% (70/155)\u001b[K\rremote: Counting objects:  46% (72/155)\u001b[K\rremote: Counting objects:  47% (73/155)\u001b[K\rremote: Counting objects:  48% (75/155)\u001b[K\rremote: Counting objects:  49% (76/155)\u001b[K\rremote: Counting objects:  50% (78/155)\u001b[K\rremote: Counting objects:  51% (80/155)\u001b[K\rremote: Counting objects:  52% (81/155)\u001b[K\rremote: Counting objects:  53% (83/155)\u001b[K\rremote: Counting objects:  54% (84/155)\u001b[K\rremote: Counting objects:  55% (86/155)\u001b[K\rremote: Counting objects:  56% (87/155)\u001b[K\rremote: Counting objects:  57% (89/155)\u001b[K\rremote: Counting objects:  58% (90/155)\u001b[K\rremote: Counting objects:  59% (92/155)\u001b[K\rremote: Counting objects:  60% (93/155)\u001b[K\rremote: Counting objects:  61% (95/155)\u001b[K\rremote: Counting objects:  62% (97/155)\u001b[K\rremote: Counting objects:  63% (98/155)\u001b[K\rremote: Counting objects:  64% (100/155)\u001b[K\rremote: Counting objects:  65% (101/155)\u001b[K\rremote: Counting objects:  66% (103/155)\u001b[K\rremote: Counting objects:  67% (104/155)\u001b[K\rremote: Counting objects:  68% (106/155)\u001b[K\rremote: Counting objects:  69% (107/155)\u001b[K\rremote: Counting objects:  70% (109/155)\u001b[K\rremote: Counting objects:  71% (111/155)\u001b[K\rremote: Counting objects:  72% (112/155)\u001b[K\rremote: Counting objects:  73% (114/155)\u001b[K\rremote: Counting objects:  74% (115/155)\u001b[K\rremote: Counting objects:  75% (117/155)\u001b[K\rremote: Counting objects:  76% (118/155)\u001b[K\rremote: Counting objects:  77% (120/155)\u001b[K\rremote: Counting objects:  78% (121/155)\u001b[K\rremote: Counting objects:  79% (123/155)\u001b[K\rremote: Counting objects:  80% (124/155)\u001b[K\rremote: Counting objects:  81% (126/155)\u001b[K\rremote: Counting objects:  82% (128/155)\u001b[K\rremote: Counting objects:  83% (129/155)\u001b[K\rremote: Counting objects:  84% (131/155)\u001b[K\rremote: Counting objects:  85% (132/155)\u001b[K\rremote: Counting objects:  86% (134/155)\u001b[K\rremote: Counting objects:  87% (135/155)\u001b[K\rremote: Counting objects:  88% (137/155)\u001b[K\rremote: Counting objects:  89% (138/155)\u001b[K\rremote: Counting objects:  90% (140/155)\u001b[K\rremote: Counting objects:  91% (142/155)\u001b[K\rremote: Counting objects:  92% (143/155)\u001b[K\rremote: Counting objects:  93% (145/155)\u001b[K\rremote: Counting objects:  94% (146/155)\u001b[K\rremote: Counting objects:  95% (148/155)\u001b[K\rremote: Counting objects:  96% (149/155)\u001b[K\rremote: Counting objects:  97% (151/155)\u001b[K\rremote: Counting objects:  98% (152/155)\u001b[K\rremote: Counting objects:  99% (154/155)\u001b[K\rremote: Counting objects: 100% (155/155)\u001b[K\rremote: Counting objects: 100% (155/155), done.\u001b[K\n","remote: Compressing objects: 100% (108/108), done.\u001b[K\n","remote: Total 5191 (delta 88), reused 105 (delta 47), pack-reused 5036\u001b[K\n","Receiving objects: 100% (5191/5191), 50.67 MiB | 34.85 MiB/s, done.\n","Resolving deltas: 100% (3394/3394), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6GD9NxaKjI8g","colab_type":"code","colab":{}},"source":["import json\n","import random\n","import pickle\n","combine_cq =[]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsHpGhQlWj6s","colab_type":"code","colab":{}},"source":["from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKv48sc1jnCt","colab_type":"code","colab":{}},"source":["with open(\"SQuAD-explorer/dataset/train-v2.0.json\",\"rb\") as outfile:\n","  squad_train = json.load(outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Jr66Cm0jwoc","colab_type":"code","colab":{}},"source":["with open(\"SQuAD-explorer/dataset/dev-v2.0.json\",\"rb\") as outfile:\n","  squad_dev = json.load(outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRIRcR03oAS9","colab_type":"code","colab":{}},"source":["for i in range(len(squad_train['data'])):\n","  for j in range(len(squad_train['data'][i]['paragraphs'])):\n","    for k in range(len(squad_train['data'][i]['paragraphs'][j]['qas'])):\n","      combine_cq.append({\n","          \"question\":squad_train['data'][i]['paragraphs'][j]['qas'][k]['question'],\n","          \"context\":squad_train['data'][i]['paragraphs'][j]['context']\n","          \n","          \n","      })\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jh4xkWn2pR17","colab_type":"code","outputId":"d4707ebc-d98f-4064-a3ea-41b6d271e41d","executionInfo":{"status":"ok","timestamp":1573453384591,"user_tz":-330,"elapsed":1585,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import random\n","random.shuffle(combine_cq)\n","combine_cq[4]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': \"From Elizabeth's birth onwards, the British Empire continued its transformation into the Commonwealth of Nations. By the time of her accession in 1952, her role as head of multiple independent states was already established. In 1953, the Queen and her husband embarked on a seven-month round-the-world tour, visiting 13 countries and covering more than 40,000 miles by land, sea and air. She became the first reigning monarch of Australia and New Zealand to visit those nations. During the tour, crowds were immense; three-quarters of the population of Australia were estimated to have seen her. Throughout her reign, the Queen has made hundreds of state visits to other countries and tours of the Commonwealth; she is the most widely travelled head of state.\",\n"," 'question': \"What entity did the British Empire slowly become during Elizabeth's reign?\"}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"hfa48QVhVIfh","colab_type":"code","outputId":"99d07f17-bcc5-41ab-a17b-bbe32be747e8","executionInfo":{"status":"ok","timestamp":1573453419806,"user_tz":-330,"elapsed":1792,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(combine_cq)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500079"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"5zIg5CEbpdYE","colab_type":"code","colab":{}},"source":["for i in range(len(squad_dev['data'])):\n","  for j in range(len(squad_dev['data'][i]['paragraphs'])):\n","    for k in range(len(squad_dev['data'][i]['paragraphs'][j]['qas'])):\n","      combine_cq.append({\n","          \"question\":squad_dev['data'][i]['paragraphs'][j]['qas'][k]['question'],\n","          \"context\":squad_dev['data'][i]['paragraphs'][j]['context']\n","          \n","          \n","      })\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXmXSKZm9NU0","colab_type":"code","colab":{}},"source":["!mkdir squad2_data\n","count = 0\n","for ex in combine_cq:\n","  with open(\"squad2_data/example_\"+str(count)+\".txt\",'w') as f:\n","    f.write(ex['context']+\"|||\"+ex['question'])\n","    count = count +1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbpLNE3G_IMh","colab_type":"code","outputId":"f04c934d-8924-4a0d-f34e-cd9821d93361","executionInfo":{"status":"ok","timestamp":1573452988108,"user_tz":-330,"elapsed":91374,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["!wget https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-11 06:14:58--  https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz\n","Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 40.112.152.16\n","Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|40.112.152.16|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1112116929 (1.0G) [application/gzip]\n","Saving to: ‘train_v2.1.json.gz’\n","\n","train_v2.1.json.gz  100%[===================>]   1.04G  12.1MB/s    in 87s     \n","\n","2019-11-11 06:16:26 (12.2 MB/s) - ‘train_v2.1.json.gz’ saved [1112116929/1112116929]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bP-YbCGnAa2T","colab_type":"code","colab":{}},"source":["!gunzip train_v2.1.json.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_2aHZ0XT1NR","colab_type":"code","colab":{}},"source":["import json\n","with open(\"train_v2.1.json\",\"rb\") as outfile:\n","  ms_train = json.load(outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1g2u62mM9Nfq","colab_type":"code","colab":{}},"source":["#combine_cq = []\n","j =0 \n","for key in list(ms_train['answers'].keys()):\n","  for element in ms_train['passages'][key]:\n","    if element['is_selected'] == 1:\n","      context = element['passage_text']\n","      #print(element)\n","      question = ms_train['query'][key]\n","      if any(word in question.lower() for word in ['who','what','when','why','how','can','?','which','whom']):\n","        if j%100==0:\n","          print(j)\n","        context = context.rstrip()\n","        answer = ms_train['answers'][key][0]\n","        combine_cq.append({\n","        \"question\":question,\n","        \"answer\" : answer,\n","        \"context\":context\n","           })\n","        j = j+1\n","          \n","          \n","          \n","          \n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjnITSjvT_WK","colab_type":"code","colab":{}},"source":["with open(\"train_story.txt\",'w') as f:\n","  for dp in combine_cq:\n","    context = dp['context'].replace(\"|||\",\"\")\n","    f.write(context.strip())\n","    f.write(\"\\n\")\n","\n","with open(\"train_summary.txt\",'w') as f:\n","  for dp in combine_cq:\n","    question = dp['question']\n","    if question.strip().endswith(\"?\") == False:\n","      question = question+\"?\"\n","    f.write(question.strip())\n","    f.write(\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkLwGSghIyW3","colab_type":"code","colab":{}},"source":["with open(\"eval_story.txt\",'w') as f:\n","  for dp in combine_cq[0:100]:\n","    context = dp['context'].replace(\"|||\",\"\")\n","    f.write(context.strip())\n","    f.write(\"\\n\")\n","\n","with open(\"eval_summary.txt\",'w') as f:\n","  for dp in combine_cq[0:100]:\n","    question = dp['question']\n","    if question.strip().endswith(\"?\") == False:\n","      question = question+\"?\"\n","    f.write(question.strip())\n","    f.write(\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qww6NZ_3JV7E","colab_type":"code","colab":{}},"source":["!cp train_story.txt /content/drive/My\\ Drive/\n","!cp train_summary.txt /content/drive/My\\ Drive/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_uU9p98Uq50","colab_type":"code","colab":{}},"source":["del ms_train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6svQw5npwc2","colab_type":"code","outputId":"3e38757d-966f-4f1b-d61f-8cac0c97531c","executionInfo":{"status":"ok","timestamp":1573019291211,"user_tz":-330,"elapsed":2608,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["!git clone https://github.com/martiansideofthemoon/squash-generation.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'squash-generation'...\n","remote: Enumerating objects: 6, done.\u001b[K\n","remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n","Receiving objects:   0% (1/375)   \rReceiving objects:   1% (4/375)   \rReceiving objects:   2% (8/375)   \rReceiving objects:   3% (12/375)   \rReceiving objects:   4% (15/375)   \rReceiving objects:   5% (19/375)   \rReceiving objects:   6% (23/375)   \rReceiving objects:   7% (27/375)   \rReceiving objects:   8% (30/375)   \rReceiving objects:   9% (34/375)   \rReceiving objects:  10% (38/375)   \rReceiving objects:  11% (42/375)   \rReceiving objects:  12% (45/375)   \rReceiving objects:  13% (49/375)   \rReceiving objects:  14% (53/375)   \rReceiving objects:  15% (57/375)   \rReceiving objects:  16% (60/375)   \rReceiving objects:  17% (64/375)   \rReceiving objects:  18% (68/375)   \rReceiving objects:  19% (72/375)   \rReceiving objects:  20% (75/375)   \rReceiving objects:  21% (79/375)   \rReceiving objects:  22% (83/375)   \rReceiving objects:  23% (87/375)   \rReceiving objects:  24% (90/375)   \rReceiving objects:  25% (94/375)   \rReceiving objects:  26% (98/375)   \rReceiving objects:  27% (102/375)   \rReceiving objects:  28% (105/375)   \rReceiving objects:  29% (109/375)   \rReceiving objects:  30% (113/375)   \rReceiving objects:  31% (117/375)   \rReceiving objects:  32% (120/375)   \rReceiving objects:  33% (124/375)   \rReceiving objects:  34% (128/375)   \rReceiving objects:  35% (132/375)   \rReceiving objects:  36% (135/375)   \rReceiving objects:  37% (139/375)   \rReceiving objects:  38% (143/375)   \rReceiving objects:  39% (147/375)   \rReceiving objects:  40% (150/375)   \rReceiving objects:  41% (154/375)   \rReceiving objects:  42% (158/375)   \rReceiving objects:  43% (162/375)   \rReceiving objects:  44% (165/375)   \rReceiving objects:  45% (169/375)   \rReceiving objects:  46% (173/375)   \rReceiving objects:  47% (177/375)   \rReceiving objects:  48% (180/375)   \rremote: Total 375 (delta 1), reused 1 (delta 0), pack-reused 369\u001b[K\n","Receiving objects:  49% (184/375)   \rReceiving objects:  50% (188/375)   \rReceiving objects:  51% (192/375)   \rReceiving objects:  52% (195/375)   \rReceiving objects:  53% (199/375)   \rReceiving objects:  54% (203/375)   \rReceiving objects:  55% (207/375)   \rReceiving objects:  56% (210/375)   \rReceiving objects:  57% (214/375)   \rReceiving objects:  58% (218/375)   \rReceiving objects:  59% (222/375)   \rReceiving objects:  60% (225/375)   \rReceiving objects:  61% (229/375)   \rReceiving objects:  62% (233/375)   \rReceiving objects:  63% (237/375)   \rReceiving objects:  64% (240/375)   \rReceiving objects:  65% (244/375)   \rReceiving objects:  66% (248/375)   \rReceiving objects:  67% (252/375)   \rReceiving objects:  68% (255/375)   \rReceiving objects:  69% (259/375)   \rReceiving objects:  70% (263/375)   \rReceiving objects:  71% (267/375)   \rReceiving objects:  72% (270/375)   \rReceiving objects:  73% (274/375)   \rReceiving objects:  74% (278/375)   \rReceiving objects:  75% (282/375)   \rReceiving objects:  76% (285/375)   \rReceiving objects:  77% (289/375)   \rReceiving objects:  78% (293/375)   \rReceiving objects:  79% (297/375)   \rReceiving objects:  80% (300/375)   \rReceiving objects:  81% (304/375)   \rReceiving objects:  82% (308/375)   \rReceiving objects:  83% (312/375)   \rReceiving objects:  84% (315/375)   \rReceiving objects:  85% (319/375)   \rReceiving objects:  86% (323/375)   \rReceiving objects:  87% (327/375)   \rReceiving objects:  88% (330/375)   \rReceiving objects:  89% (334/375)   \rReceiving objects:  90% (338/375)   \rReceiving objects:  91% (342/375)   \rReceiving objects:  92% (345/375)   \rReceiving objects:  93% (349/375)   \rReceiving objects:  94% (353/375)   \rReceiving objects:  95% (357/375)   \rReceiving objects:  96% (360/375)   \rReceiving objects:  97% (364/375)   \rReceiving objects:  98% (368/375)   \rReceiving objects:  99% (372/375)   \rReceiving objects: 100% (375/375)   \rReceiving objects: 100% (375/375), 397.70 KiB | 9.94 MiB/s, done.\n","Resolving deltas:   0% (0/177)   \rResolving deltas:   1% (2/177)   \rResolving deltas:   2% (4/177)   \rResolving deltas:   3% (6/177)   \rResolving deltas:  19% (34/177)   \rResolving deltas:  22% (40/177)   \rResolving deltas:  23% (42/177)   \rResolving deltas:  24% (43/177)   \rResolving deltas:  28% (51/177)   \rResolving deltas:  29% (52/177)   \rResolving deltas:  30% (54/177)   \rResolving deltas:  31% (56/177)   \rResolving deltas:  32% (57/177)   \rResolving deltas:  33% (59/177)   \rResolving deltas:  34% (61/177)   \rResolving deltas:  35% (62/177)   \rResolving deltas:  37% (67/177)   \rResolving deltas:  40% (71/177)   \rResolving deltas:  43% (77/177)   \rResolving deltas:  44% (78/177)   \rResolving deltas:  45% (80/177)   \rResolving deltas:  46% (83/177)   \rResolving deltas:  48% (85/177)   \rResolving deltas:  56% (100/177)   \rResolving deltas:  57% (101/177)   \rResolving deltas:  58% (103/177)   \rResolving deltas:  60% (107/177)   \rResolving deltas:  63% (112/177)   \rResolving deltas:  65% (116/177)   \rResolving deltas:  66% (118/177)   \rResolving deltas:  68% (121/177)   \rResolving deltas:  71% (127/177)   \rResolving deltas:  72% (128/177)   \rResolving deltas:  75% (133/177)   \rResolving deltas:  78% (139/177)   \rResolving deltas:  80% (143/177)   \rResolving deltas:  82% (146/177)   \rResolving deltas:  86% (153/177)   \rResolving deltas:  88% (156/177)   \rResolving deltas:  90% (160/177)   \rResolving deltas:  92% (163/177)   \rResolving deltas:  96% (171/177)   \rResolving deltas:  97% (172/177)   \rResolving deltas:  98% (174/177)   \rResolving deltas:  99% (176/177)   \rResolving deltas: 100% (177/177)   \rResolving deltas: 100% (177/177), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ngUzM_H4p5AP","colab_type":"code","colab":{}},"source":["!cp -r squash-generation/pytorch-pretrained-BERT/pytorch_pretrained_bert ."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDAZwALBVPOH","colab_type":"code","colab":{}},"source":["!rm -r train_data eval_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FU5psE1qIcF","colab_type":"code","colab":{}},"source":["!mkdir train_data\n","!mkdir eval_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_I-9IFGdwrCH","colab_type":"code","outputId":"8e96edd4-4824-42f3-9601-d668abfb4485","executionInfo":{"status":"ok","timestamp":1573027757961,"user_tz":-330,"elapsed":914,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["combine_cq[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': 'The ISO standard ISO 12232:2006 gives digital still camera manufacturers a choice of five different techniques for determining the exposure index rating at each sensitivity setting provided by a particular camera model. Three of the techniques in ISO 12232:2006 are carried over from the 1998 version of the standard, while two new techniques allowing for measurement of JPEG output files are introduced from CIPA DC-004. Depending on the technique selected, the exposure index rating can depend on the sensor sensitivity, the sensor noise, and the appearance of the resulting image. The standard specifies the measurement of light sensitivity of the entire digital camera system and not of individual components such as digital sensors, although Kodak has reported using a variation to characterize the sensitivity of two of their sensors in 2001.',\n"," 'question': 'What techniques did CIPA DC-004 provide?'}"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"poAoqAsXwc1n","colab_type":"code","colab":{}},"source":["for i in range(len(combine_cq)):\n","  if combine_cq[i]['context'].strip()==\"\":\n","    print(combine_cq[i])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ekWh5zHp6zn","colab_type":"code","colab":{}},"source":["for i in range(len(combine_cq)):\n","  file_name = \"train_data/example\"+str(i)+\".txt\"\n","  with open(file_name,'w') as f:\n","    f.write(str(combine_cq[i]['context'].strip().replace(\"|||\",\"\"))+\"|||\"+str(combine_cq[i]['question'].strip()))\n","  f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RioaB7FvqCxQ","colab_type":"code","colab":{}},"source":["for i in range(100):\n","  file_name = \"eval_data/example\"+str(i)+\".txt\"\n","  with open(file_name,'w') as f:\n","    f.write(str(combine_cq[i]['context'].strip())+\"|||\"+str(combine_cq[i]['question'].strip()))\n","  f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmrBP5-rVh0O","colab_type":"code","colab":{}},"source":["!zip -r eval_data.zip eval_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zzn_9jwKqgG-","colab_type":"code","colab":{}},"source":["!pip install -r squash-generation/requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAaBVUQwq6rN","colab_type":"code","colab":{}},"source":["import os\n","import math\n","import logging\n","from pprint import pformat\n","from collections import defaultdict\n","from itertools import chain\n","\n","import torch\n","from torch.nn.parallel import DistributedDataParallel\n","from torch.utils.data import DataLoader, TensorDataset\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from pytorch_pretrained_bert import (OpenAIAdam, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n","                                     GPT2LMHeadModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n","\n","#from arguments import parser\n","#from dataloader import get_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJwGcYxhq_a4","colab_type":"code","colab":{}},"source":["SPECIAL_TOKENS = [\n","    \"<bos>\", \"<eos>\", \"<context>\",\"<question>\",\"<answer>\",\"<pad>\"\n","]\n","MODEL_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","bos, eos, context,question,answer,pad = \\\n","          tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","tokenizer.set_special_tokens(SPECIAL_TOKENS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAgQzqrDeZlB","colab_type":"code","colab":{}},"source":["#answer first \n","\n","from torch.utils.data import Dataset, DataLoader\n","def get_example(txt):\n","  inst = {}\n","  try:\n","    context,question = txt.strip().split(\"|||\")\n","  except:\n","    print(txt)\n","  toks1 = tokenizer.tokenize(context.strip())\n","  toks2 = tokenizer.tokenize(question.strip())\n"," \n","  if (len(toks1) + len(toks2))< 1019:\n","    inst['context'] = tokenizer.convert_tokens_to_ids(toks1)\n","    inst['question'] = tokenizer.convert_tokens_to_ids(toks2)\n","  else:\n","    #count = count+1\n","    max_len_for_article = 1019 - len(toks2)\n","    inst['context'] = tokenizer.convert_tokens_to_ids(toks1)[:max_len_for_article]\n","    inst['question'] = tokenizer.convert_tokens_to_ids(toks2)\n","    \n","  bos, eos, context,question,answer,pad = \\\n","          tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","  instance = {}\n","  \n","  sequence = [bos] + inst['context'] \n","  token_types = [context for _ in range(len(inst['context'])+1)]\n","  lm_labels = [-1 for _ in range(len(inst['context'])+1)]\n","  \n","  \n","  sequence.extend([question]+inst['question']+[eos])\n","  token_types.extend([question for _ in range(len(inst['question'])+2)])\n","  lm_labels.extend([question]+inst['question']+[eos])\n","  \n","  \n","  \n","  \n","  #instance[\"input_ids\"] = torch.tensor(sequence)\n","  #instance[\"token_type_ids\"] = torch.tensor(token_types)\n","  #instance[\"lm_labels\"] = torch.tensor(lm_labels)\n","  return torch.tensor(sequence),torch.tensor(token_types), torch.tensor(lm_labels)\n","  \n","\n","def pad_and_sort_batch(DataLoaderBatch):\n","  batch_size = len(DataLoaderBatch)\n","  batch_split = list(zip(*DataLoaderBatch))\n","  \n","  seqs,types,labels = batch_split[0],batch_split[1],batch_split[2]\n","  seqs = [seq.tolist() for seq in seqs]\n","  types = [typ.tolist() for typ in types] \n","  labels = [lb.tolist() for lb in labels]\n","  \n","  max_len = max([len(seq) for seq in seqs])\n","  \n","  \n","  seqs = [seq+[pad]*(max_len-len(seq)) for seq in seqs]\n","  types = [seq+[pad]*(max_len-len(seq)) for seq in types] \n","  labels = [seq+[pad]*(max_len-len(seq)) for seq in labels]\n","  \n","  return torch.tensor(seqs),torch.tensor(types),torch.tensor(labels)\n","  \n","  \n","\n","class MyDataset(Dataset):\n","  def __init__(self,path):\n","    \n","    self.path = path\n","    self.data_files = os.listdir(path)\n","\n","  def __getitem__(self, idx):\n","    with open(self.path+\"/\"+self.data_files[idx]) as f:\n","      txt = \" \".join(f.readlines())\n","    return get_example(txt)\n","\n","  def __len__(self):\n","      return len(self.data_files)\n","    \n","train_dataset = MyDataset(\"train_data\")\n","eval_dataset = MyDataset(\"eval_data\")\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4,collate_fn = pad_and_sort_batch)\n","eval_loader =  DataLoader(eval_dataset, batch_size=2, shuffle=False, num_workers=4,collate_fn = pad_and_sort_batch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmVWXV5kEN48","colab_type":"code","outputId":"1cc059d1-54c4-4d96-ed66-23cc7d561068","executionInfo":{"status":"ok","timestamp":1573453641418,"user_tz":-330,"elapsed":25740,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"awBfgzCuAIIc","colab_type":"code","colab":{}},"source":["it = iter(train_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmM93j5kAL1Q","colab_type":"code","colab":{}},"source":["x,y,z= next(it)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WrtWAMz1P7b","colab_type":"code","outputId":"02e75ed9-10ea-4021-9dfe-28b5a59d44dd","executionInfo":{"status":"ok","timestamp":1573029001200,"user_tz":-330,"elapsed":870,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["combine_cq[20458]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'answer': 'For Clinton.',\n"," 'context': 'AFL-CIO announces support for Clinton, slams Trump as an ‘unstable charlatan’. Be the first to know about new stories from PowerPost. Sign up to follow, and we’ll e-mail you free updates as they’re published.',\n"," 'question': 'what did the afl support'}"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"f_sCdrXb-Nt6","colab_type":"code","colab":{}},"source":["from tqdm import tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8H18YSo8IOq","colab_type":"code","outputId":"1dc404c4-0791-452d-e042-b7d9f6cd1257","executionInfo":{"status":"ok","timestamp":1571555951427,"user_tz":-330,"elapsed":270488,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["for step, (x, y,z) in tqdm_notebook(enumerate(train_loader)): \n","  if x.size()[1]>1024  or y.size()[1]>1024 or z.size()[1]>1024:\n","    print(tokenizer.decode(x[0].tolist()))\n","    print(step)\n","    break"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"773bb40d460d409d8fcfd221b1680382","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"cLNGQ6q8xceM","colab_type":"code","outputId":"86838283-8c5e-47cc-a2b3-bc4913b67cdc","executionInfo":{"status":"ok","timestamp":1571554607306,"user_tz":-330,"elapsed":602,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["txt"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Pagan's collapse was followed by 250 years of political fragmentation that lasted well into the 16th century. Like the Burmans four centuries earlier, Shan migrants who arrived with the Mongol invasions stayed behind. Several competing Shan States came to dominate the entire northwestern to eastern arc surrounding the Irrawaddy valley. The valley too was beset with petty states until the late 14th century when two sizeable powers, Ava Kingdom and Hanthawaddy Kingdom, emerged. In the west, a politically fragmented Arakan was under competing influences of its stronger neighbours until the Kingdom of Mrauk U unified the Arakan coastline for the first time in 1437.|||What is the name of the kingdom that became dominant along the coastline of Myanmar? |||Kingdom of Mrauk U\"]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"yz3KZT50v12T","colab_type":"code","colab":{}},"source":["args = {'distributed':False,'train_batch_size':2,'valid_batch_size':2,\n","       \"device\":\"cuda\",\n","        \"model_checkpoint\":\"gpt2\",\n","        \"lr\":6.25e-5,\n","        \"local_rank\":-1,\n","        \"n_epochs\":15,\n","        \"max_norm\":1.0,\n","        \"gradient_accumulation_steps\":8,\n","        \"output_dir\":\"logs\",\"fp16\":\"\"\n","        \n","       }\n","from collections import namedtuple\n","args = namedtuple(\"GenericDict\",args.keys())(**args)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yy6RTrxvVl1O","colab_type":"code","outputId":"4b068e99-04c0-45b0-d4ea-30a023f2306e","executionInfo":{"status":"ok","timestamp":1571493971181,"user_tz":-330,"elapsed":2551,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["!ps -ef|grep python"],"execution_count":0,"outputs":[{"output_type":"stream","text":["root          26      10  1 10:23 ?        00:02:43 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n","root        3038      26  3 14:04 ?        00:00:02 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-7b1c762a-b387-4a0f-b879-c0d4c89c4ab5.json\n","root        3092    3038  0 14:06 ?        00:00:00 /bin/bash -c ps -ef|grep python\n","root        3094    3092  0 14:06 ?        00:00:00 grep python\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qP_jXpnEv2L9","colab_type":"code","outputId":"edc93d39-5f48-4e42-9300-b1148605bf8b","executionInfo":{"status":"ok","timestamp":1573021598875,"user_tz":-330,"elapsed":12842,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#torch.cuda.empty()\n","model_class = GPT2LMHeadModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTLMHeadModel\n","model = model_class.from_pretrained(\"gpt2\")\n","tokenizer.set_special_tokens(SPECIAL_TOKENS)\n","model.set_num_special_tokens(len(SPECIAL_TOKENS))\n","model.to(args.device)\n","optimizer = OpenAIAdam(model.parameters(), lr=args.lr)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["t_total value of -1 results in schedule not being applied\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VqIoytwHv3zN","colab_type":"code","colab":{}},"source":["# Training function and trainer\n","def update(engine, batch):\n","    model.train()\n","    #batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n","    \n","    \n","    lm_loss = model(batch[0].to(args.device), token_type_ids=batch[1].to(args.device),\n","                    lm_labels=batch[2].to(args.device))\n","    loss = lm_loss / args.gradient_accumulation_steps\n","    if args.fp16:\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n","    else:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n","    if engine.state.iteration % args.gradient_accumulation_steps == 0:\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    return loss.item()\n","trainer = Engine(update)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zp8oDu7fBF7X","colab_type":"code","colab":{}},"source":["trainer.run?\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7ZQ-wV1wcSg","colab_type":"code","colab":{}},"source":["# Evaluation function and evaluator (evaluator output is the input of the metrics)\n","def inference(engine, batch):\n","    model.eval()\n","    with torch.no_grad():\n","        #batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n","        #input_ids, lm_labels, token_type_ids = batch\n","        input_ids = batch[0].to(args.device)\n","        lm_labels = batch[1].to(args.device)\n","        token_type_ids = batch[2].to(args.device)\n","\n","        # logger.info(tokenizer.decode(input_ids[0, :].tolist()))\n","        model_outputs = model(input_ids, token_type_ids=token_type_ids)\n","        lm_logits = model_outputs[0]\n","\n","        lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n","        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n","\n","        return lm_logits_flat_shifted, lm_labels_flat_shifted\n","evaluator = Engine(inference)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgBBUfCQwlJD","colab_type":"code","colab":{}},"source":["trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(eval_loader))\n","if args.n_epochs < 1:\n","    trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(eval_loader))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdsPSFJ2wxfM","colab_type":"code","outputId":"7b9d4ba3-8c3c-4e0e-faf7-cfa7f18afd59","executionInfo":{"status":"error","timestamp":1573026903593,"user_tz":-330,"elapsed":5204029,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["07e92675153646c3b41305e3a69beb42"]}},"source":["def average_distributed_scalar(scalar, args):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if args.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","  \n","# Linearly decrease the learning rate from lr to zero\n","scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n","trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","\n","# Prepare metrics - note how we compute distributed metrics\n","RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n","metrics = {\n","    \"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1))\n","}\n","metrics.update({\n","    \"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args)\n","})\n","metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n","for name, metric in metrics.items():\n","    metric.attach(evaluator, name)\n","\n","# On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n","if args.local_rank in [-1, 0]:\n","    pbar = ProgressBar(persist=True)\n","    pbar.attach(trainer, metric_names=[\"loss\"])\n","    evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n","\n","    tb_logger = TensorboardLogger(log_dir=args.output_dir)\n","    tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n","    tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n","    tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), another_engine=trainer), event_name=Events.EPOCH_COMPLETED)\n","\n","    checkpoint_handler = ModelCheckpoint(\"logs\", 'checkpoint', save_interval=10000, n_saved=5)\n","    trainer.add_event_handler(Events.ITERATION_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" take care of distributed encapsulation\n","\n","    #torch.save(args, tb_logger.writer.log_dir + '/model_training_args.bin')\n","    getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))\n","    tokenizer.save_vocabulary(tb_logger.writer.log_dir)\n","\n","# Run the training\n","trainer.run(train_loader, max_epochs=args.n_epochs)\n","\n","# On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n","if args.local_rank in [-1, 0] and args.n_epochs > 0:\n","    os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n","    tb_logger.close()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07e92675153646c3b41305e3a69beb42","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=250040), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_xMEgOFmNy4i","colab_type":"code","outputId":"12f685c0-d136-40cf-83a8-fdb4cf7abae0","executionInfo":{"status":"ok","timestamp":1573021480089,"user_tz":-330,"elapsed":21078,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uRmh7LPI7nPc","colab_type":"code","outputId":"e7e2f3f0-f125-4e21-8136-1501cd327768","executionInfo":{"status":"ok","timestamp":1571501477061,"user_tz":-330,"elapsed":863,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["combine_cq[30404]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'answer': 'main repository for information',\n"," 'context': 'The Criminal Justice Information Services (CJIS) Division, is located in Clarksburg, West Virginia. Organized beginning in 1991, the office opened in 1995 as the youngest agency division. The complex is the length of three football fields. It provides a main repository for information in various data systems. Under the roof of the CJIS are the programs for the National Crime Information Center (NCIC), Uniform Crime Reporting (UCR), Fingerprint Identification, Integrated Automated Fingerprint Identification System (IAFIS), NCIC 2000, and the National Incident-Based Reporting System (NIBRS). Many state and local agencies use these data systems as a source for their own investigations and contribute to the database using secure communications. FBI provides these tools of sophisticated identification and information services to local, state, federal, and international law enforcement agencies.',\n"," 'question': 'What purpose does the CJIS serve?'}"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"45DN2ahRxOge","colab_type":"code","outputId":"c4b7b958-81d6-41e5-faed-c845fd5d830d","executionInfo":{"status":"ok","timestamp":1571554437498,"user_tz":-330,"elapsed":1160,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["16770*2"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["33540"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"aZkq30Xb8aSo","colab_type":"code","colab":{}},"source":["!cp eval_data.zip /content/drive/My\\ Drive/gpt2_squad/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"900zpQGr749A","colab_type":"code","outputId":"ee924142-e3af-4e4d-ac37-372fa56c29f2","executionInfo":{"status":"ok","timestamp":1573453760875,"user_tz":-330,"elapsed":3535,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["!wget https://github.com/asyml/texar/archive/v0.1.0.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-11 06:29:19--  https://github.com/asyml/texar/archive/v0.1.0.zip\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://codeload.github.com/asyml/texar/zip/v0.1.0 [following]\n","--2019-11-11 06:29:19--  https://codeload.github.com/asyml/texar/zip/v0.1.0\n","Resolving codeload.github.com (codeload.github.com)... 192.30.253.120\n","Connecting to codeload.github.com (codeload.github.com)|192.30.253.120|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/zip]\n","Saving to: ‘v0.1.0.zip’\n","\n","\rv0.1.0.zip              [<=>                 ]       0  --.-KB/s               \rv0.1.0.zip              [ <=>                ] 673.60K  --.-KB/s    in 0.1s    \n","\n","2019-11-11 06:29:19 (5.09 MB/s) - ‘v0.1.0.zip’ saved [689770]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Thpss4L2JvtC","colab_type":"code","colab":{}},"source":["!unzip v0.1.0.zip "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBvI_w6zJyLT","colab_type":"code","colab":{}},"source":["!mv texar-0.1.0 texar_repo"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaUuY1RQJ0sz","colab_type":"code","outputId":"c5f1e32e-3601-4787-f4e5-40fc0ac6b57b","executionInfo":{"status":"ok","timestamp":1573457510585,"user_tz":-330,"elapsed":3676,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import sys\n","\n","#!test -d texar_repo || git clone https://github.com/asyml/texar.git texar_repo\n","if not 'texar_repo' in sys.path:\n","  sys.path += ['texar_repo']\n","!pip install funcsigs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (1.0.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Budqs3bJ4bV","colab_type":"code","outputId":"f0f2bd43-690a-4157-d27c-b9944e626b30","executionInfo":{"status":"ok","timestamp":1573457514917,"user_tz":-330,"elapsed":3709,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":831}},"source":["import os\n","import csv\n","import collections\n","import sys\n","from texar_repo.examples.bert.utils import data_utils, model_utils, tokenization\n","import importlib\n","import tensorflow as tf\n","import texar as tx \n","from texar_repo.examples.bert import config_classifier as config_downstream\n","from texar_repo.texar.utils import transformer_utils\n","from texar_repo.examples.transformer.utils import data_utils, utils\n","from texar_repo.examples.transformer.bleu_tool import bleu_wrapper"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:628: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:681: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:682: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1157: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1158: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1159: The name tf.layers.Conv3D is deprecated. Please use tf.compat.v1.layers.Conv3D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1160: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1161: The name tf.layers.Conv3DTranspose is deprecated. Please use tf.compat.v1.layers.Conv3DTranspose instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1162: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1163: The name tf.layers.Dropout is deprecated. Please use tf.compat.v1.layers.Dropout instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1164: The name tf.layers.Flatten is deprecated. Please use tf.compat.v1.layers.Flatten instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1166: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1167: The name tf.layers.MaxPooling3D is deprecated. Please use tf.compat.v1.layers.MaxPooling3D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1168: The name tf.layers.SeparableConv2D is deprecated. Please use tf.compat.v1.layers.SeparableConv2D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1169: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1171: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1172: The name tf.layers.AveragePooling3D is deprecated. Please use tf.compat.v1.layers.AveragePooling3D instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/optimization.py:482: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GgqhslBFJ7GA","colab_type":"code","colab":{}},"source":["dcoder_config = {\n","    'dim': 768,\n","    'num_blocks': 6,\n","    'multihead_attention': {\n","        'num_heads': 8,\n","        'output_dim': 768\n","        # See documentation for more optional hyperparameters\n","    },\n","    'position_embedder_hparams': {\n","        'dim': 768\n","    },\n","    'initializer': {\n","        'type': 'variance_scaling_initializer',\n","        'kwargs': {\n","            'scale': 1.0,\n","            'mode': 'fan_avg',\n","            'distribution': 'uniform',\n","        },\n","    },\n","    'poswise_feedforward': tx.modules.default_transformer_poswise_net_hparams(\n","        output_dim=768)\n","}\n","\n","loss_label_confidence = 0.9\n","\n","random_seed = 1234\n","beam_width = 1\n","alpha = 0.6\n","hidden_dim = 768\n","\n","\n","opt = {\n","    'optimizer': {\n","        'type': 'AdamOptimizer',\n","        'kwargs': {\n","            'beta1': 0.9,\n","            'beta2': 0.997,\n","            'epsilon': 1e-9\n","        }\n","    }\n","}\n","\n","\n","lr = {\n","    'learning_rate_schedule': 'constant.linear_warmup.rsqrt_decay.rsqrt_depth',\n","    'lr_constant': 2 * (hidden_dim ** -0.5),\n","    'static_lr': 1e-3,\n","    'warmup_steps': 15000,\n","}\n","\n","bos_token_id =101\n","eos_token_id = 102\n","\n","model_dir= \"./models\"\n","run_mode= \"train_and_evaluate\"\n","batch_size = 32\n","test_batch_size = 32\n","\n","max_train_epoch = 20\n","display_steps = 100\n","eval_steps = 1000000\n","\n","max_decoding_length = 40\n","\n","max_seq_length_src = 512\n","max_seq_length_tgt = 40\n","\n","bert_pretrain_dir = 'bert_pretrained_models/uncased_L-12_H-768_A-12'\n","#config"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMqGx1WlJ_Vw","colab_type":"code","outputId":"1db5897f-27f8-4ec8-8577-e6bfcf515d8a","executionInfo":{"status":"ok","timestamp":1573456739433,"user_tz":-330,"elapsed":18018,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["!mkdir bert_pretrained_models\n","!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip -P bert_pretrained_models/;\n","!unzip bert_pretrained_models/uncased_L-12_H-768_A-12.zip -d bert_pretrained_models/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘bert_pretrained_models’: File exists\n","--2019-11-11 07:18:44--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.31.128, 2607:f8b0:400c:c01::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.31.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 407727028 (389M) [application/zip]\n","Saving to: ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip.1’\n","\n","uncased_L-12_H-768_ 100%[===================>] 388.84M   141MB/s    in 2.8s    \n","\n","2019-11-11 07:18:47 (141 MB/s) - ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n","\n","Archive:  bert_pretrained_models/uncased_L-12_H-768_A-12.zip\n","replace bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CiG1lv8KKIHA","colab_type":"code","colab":{}},"source":["def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal\n","    # percent of tokens from each, since if one sequence is very short then\n","    # each token that's truncated likely contains more information than a\n","    # longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()\n","\n","class InputExample():\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence.\n","                For single sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second\n","                sequence. Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","                specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.src_txt = text_a\n","        self.tgt_txt = text_b\n","        \n","class InputFeatures():\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, src_input_ids,src_input_mask,src_segment_ids,tgt_input_ids,tgt_input_mask,tgt_labels):\n","        self.src_input_ids = src_input_ids\n","        self.src_input_mask = src_input_mask\n","        self.src_segment_ids = src_segment_ids\n","        self.tgt_input_ids = tgt_input_ids\n","        self.tgt_input_mask = tgt_input_mask \n","        self.tgt_labels = tgt_labels\n","        \n","       \n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","    @classmethod\n","    def _read_tsv(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with tf.gfile.Open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","            lines = []\n","            i = 0\n","            for line in reader:\n","                lines.append(line)\n","        return lines\n","\n","\n","    @classmethod\n","    def _read_file(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\n\", quotechar=quotechar)\n","            lines = []\n","            i = 0\n","            for line in reader:\n","                lines.append(line)\n","        return lines\n","      \n","      \n","class CNNDailymail(DataProcessor):\n","    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_file(os.path.join(data_dir, \"train_story.txt\")),self._read_file(os.path.join(data_dir, \"train_summary.txt\")),\n","            \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_file(os.path.join(data_dir, \"eval_story.txt\")),self._read_file(os.path.join(data_dir, \"eval_summary.txt\")),\n","            \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_file(os.path.join(data_dir, \"eval_story.txt\")),self._read_file(os.path.join(data_dir, \"eval_summary.txt\")),\n","            \"test\")\n","\n","    def _create_examples(self, src_lines,tgt_lines,set_type):\n","        examples = [] \n","        for i,data in enumerate(zip(src_lines,tgt_lines)):\n","            guid = \"%s-%s\" % (set_type, i)\n","            if set_type == \"test\" and i == 0:\n","                continue\n","            else:\n","                #print(data)\n","                if len(data[0])==0 or len(data[1])==0:\n","                  continue\n","                src_lines = tokenization.convert_to_unicode(data[0][0])\n","                tgt_lines = tokenization.convert_to_unicode(data[1][0])\n","                examples.append(InputExample(guid=guid, text_a=src_lines,\n","                                         text_b=tgt_lines))\n","        return examples\n","  \n","  \n","def file_based_convert_examples_to_features(\n","        examples, max_seq_length_src,max_seq_length_tgt,tokenizer, output_file):\n","    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n","\n","    writer = tf.python_io.TFRecordWriter(output_file)\n","\n","    for (ex_index, example) in enumerate(examples):\n","        #print(\"ex_index\",ex_index)\n","\n","        if (ex_index+1) %1000 == 0 :\n","          print(\"------------processed..{}...examples\".format(ex_index))\n","          \n","        feature = convert_single_example(ex_index, example,\n","                                         max_seq_length_src,max_seq_length_tgt,tokenizer)\n","\n","        def create_int_feature(values):\n","            return tf.train.Feature(\n","                int64_list=tf.train.Int64List(value=list(values)))\n","\n","        features = collections.OrderedDict()\n","        features[\"src_input_ids\"] = create_int_feature(feature.src_input_ids)\n","        features[\"src_input_mask\"] = create_int_feature(feature.src_input_mask)\n","        features[\"src_segment_ids\"] = create_int_feature(feature.src_segment_ids)\n","\n","        features[\"tgt_input_ids\"] = create_int_feature(feature.tgt_input_ids)\n","        features[\"tgt_input_mask\"] = create_int_feature(feature.tgt_input_mask)\n","        features['tgt_labels'] = create_int_feature(feature.tgt_labels)\n","        \n","        \n","        \n","        #print(feature.tgt_labels)\n","        \n","\n","        tf_example = tf.train.Example(\n","            features=tf.train.Features(feature=features))\n","        writer.write(tf_example.SerializeToString())\n","    writer.close()\n","\n","\n","def convert_single_example(ex_index, example, max_seq_length_src,max_seq_length_tgt,\n","                           tokenizer):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","    \"\"\"\n","    label_map = {}\n","    for (i, label) in enumerate(label_list):\n","        label_map[label] = i\n","    \"\"\"\n","    #tokens_a= example.src_txt\n","    tokens_a = tokenizer.tokenize(example.src_txt)\n","    \n","    \n","    #tokens_a = tokenizer.tokenize(example.src_txt)\n","    tokens_b = tokenizer.tokenize(example.tgt_txt)\n","\n","\n","    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","    # length is less than the specified length.\n","    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","    \n","    \n","    if len(tokens_b) > max_seq_length_tgt - 2:\n","            tokens_b = tokens_b[0:(max_seq_length_tgt - 2)]\n","        \n","    if len(tokens_a) > max_seq_length_src - 2:\n","            tokens_a = tokens_a[0:(max_seq_length_src - 2)]\n","\n","    \n","    tokens_src = []\n","    segment_ids_src = []\n","    tokens_src.append(\"[CLS]\")\n","    segment_ids_src.append(0)\n","    for token in tokens_a:\n","        tokens_src.append(token)\n","        segment_ids_src.append(0)\n","    tokens_src.append(\"[SEP]\")\n","    segment_ids_src.append(0)\n","    \n","    \n","      \n","  \n","\n","    tokens_tgt = []\n","    segment_ids_tgt = []\n","    tokens_tgt.append(\"[CLS]\")\n","    #segment_ids_tgt.append(0)\n","    for token in tokens_b:\n","        tokens_tgt.append(token)\n","        #segment_ids_tgt.append(0)\n","    tokens_tgt.append(\"[SEP]\")\n","    #segment_ids_tgt.append(0)\n","\n","    input_ids_src = tokenizer.convert_tokens_to_ids(tokens_src)\n","   \n","    \n","\n","    input_ids_tgt = tokenizer.convert_tokens_to_ids(tokens_tgt)\n","    \n","    \n","\n","    labels_tgt = input_ids_tgt[1:]\n","    \n","    #Adding begiining and end token\n","    input_ids_tgt = input_ids_tgt[:-1] \n","    \n","    input_mask_src = [1] * len(input_ids_src)\n","\n","\n","    input_mask_tgt = [1] * len(input_ids_tgt)\n","    \n","    #print(len(input_ids_tgt))\n","    #print(len(input_mask_tgt))\n","    #print(len(labels_tgt))\n","    #print(len(segment_ids_tgt))\n","    \n","    while len(input_ids_src) < max_seq_length_src:\n","        input_ids_src.append(0)\n","        input_mask_src.append(0)\n","        segment_ids_src.append(0)\n","\n","    while len(input_ids_tgt) < max_seq_length_tgt:\n","        input_ids_tgt.append(0)\n","        input_mask_tgt.append(0)\n","        segment_ids_tgt.append(0)\n","        labels_tgt.append(0)\n","\n","    feature = InputFeatures( src_input_ids=input_ids_src,src_input_mask=input_mask_src,src_segment_ids=segment_ids_src,\n","        tgt_input_ids=input_ids_tgt,tgt_input_mask=input_mask_tgt,tgt_labels=labels_tgt)\n","\n","    \n","    return feature\n","\n","\n","def file_based_input_fn_builder(input_file, max_seq_length_src,max_seq_length_tgt, is_training,\n","                                drop_remainder, is_distributed=False):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    name_to_features = {\n","        \"src_input_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n","        \"src_input_mask\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n","        \"src_segment_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n","        \"tgt_input_ids\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n","        \"tgt_input_mask\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n","        \"tgt_labels\" : tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n","        \n","        \n","    }\n","\n","    def _decode_record(record, name_to_features):\n","        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","        example = tf.parse_single_example(record, name_to_features)\n","        print(example)\n","        print(example.keys())\n","\n","        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","        # So cast all int64 to int32.\n","        for name in list(example.keys()):\n","            t = example[name]\n","            if t.dtype == tf.int64:\n","                t = tf.to_int32(t)\n","            example[name] = t\n","\n","        return example\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # For training, we want a lot of parallel reading and shuffling.\n","        # For eval, we want no shuffling and parallel reading doesn't matter.\n","        d = tf.data.TFRecordDataset(input_file)\n","        if is_training:\n","\n","            if is_distributed:\n","                import horovod.tensorflow as hvd\n","                tf.logging.info('distributed mode is enabled.'\n","                                'size:{} rank:{}'.format(hvd.size(), hvd.rank()))\n","                # https://github.com/uber/horovod/issues/223\n","                d = d.shard(hvd.size(), hvd.rank())\n","\n","                d = d.repeat()\n","                d = d.shuffle(buffer_size=100)\n","                d = d.apply(\n","                    tf.contrib.data.map_and_batch(\n","                        lambda record: _decode_record(record, name_to_features),\n","                        batch_size=batch_size//hvd.size(),\n","                        drop_remainder=drop_remainder))\n","            else:\n","                tf.logging.info('distributed mode is not enabled.')\n","                d = d.repeat()\n","                d = d.shuffle(buffer_size=100)\n","                d = d.apply(\n","                    tf.contrib.data.map_and_batch(\n","                        lambda record: _decode_record(record, name_to_features),\n","                        batch_size=batch_size,\n","                        drop_remainder=drop_remainder))\n","\n","        else:\n","            d = d.apply(\n","                tf.contrib.data.map_and_batch(\n","                    lambda record: _decode_record(record, name_to_features),\n","                    batch_size=batch_size,\n","                    drop_remainder=drop_remainder))\n","\n","        return d\n","    return input_fn\n","  \n","  \n","def get_dataset(processor,\n","                tokenizer,\n","                data_dir,\n","                max_seq_length_src,\n","                max_seq_length_tgt,\n","                batch_size,\n","                mode,\n","                output_dir,\n","                is_distributed=False):\n","    \"\"\"\n","    Args:\n","        processor: Data Preprocessor, must have get_lables,\n","            get_train/dev/test/examples methods defined.\n","        tokenizer: The Sentence Tokenizer. Generally should be\n","            SentencePiece Model.\n","        data_dir: The input data directory.\n","        max_seq_length: Max sequence length.\n","        batch_size: mini-batch size.\n","        model: `train`, `eval` or `test`.\n","        output_dir: The directory to save the TFRecords in.\n","    \"\"\"\n","    #label_list = processor.get_labels()\n","    if mode == 'train':\n","        #train_examples = processor.get_train_examples(data_dir)\n","        train_file = os.path.join(output_dir, \"train.tf_record\")\n","        #train_file = \"gs://answer2qa/train.tf_record\"\n","        #file_based_convert_examples_to_features(\n","        #    train_examples, max_seq_length_src,max_seq_length_tgt,\n","        #    tokenizer, train_file)\n","        dataset = file_based_input_fn_builder(\n","            input_file=train_file,\n","            max_seq_length_src=max_seq_length_src,\n","            max_seq_length_tgt =max_seq_length_tgt,\n","            is_training=True,\n","            drop_remainder=True,\n","            is_distributed=is_distributed)({'batch_size': batch_size})\n","    elif mode == 'eval':\n","        #eval_examples = processor.get_dev_examples(data_dir)\n","        eval_file = os.path.join(output_dir, \"eval.tf_record\")\n","        #eval_file = \"gs://answer2qa/eval.tf_record\"\n","        #file_based_convert_examples_to_features(\n","        #    eval_examples, max_seq_length_src,max_seq_length_tgt,\n","        #    tokenizer, eval_file)\n","        dataset = file_based_input_fn_builder(\n","            input_file=eval_file,\n","            max_seq_length_src=max_seq_length_src,\n","            max_seq_length_tgt =max_seq_length_tgt,\n","            is_training=False,\n","            drop_remainder=True,\n","            is_distributed=is_distributed)({'batch_size': batch_size})\n","    elif mode == 'test':\n","      \n","        #test_examples = processor.get_test_examples(data_dir)\n","        test_file = os.path.join(output_dir, \"predict.tf_record\")\n","        #test_file = \"gs://answer2qa/eval.tf_record\"\n","        \n","        #file_based_convert_examples_to_features(\n","        #    test_examples, max_seq_length_src,max_seq_length_tgt,\n","        #    tokenizer, test_file)\n","        dataset = file_based_input_fn_builder(\n","            input_file=test_file,\n","            max_seq_length_src=max_seq_length_src,\n","            max_seq_length_tgt =max_seq_length_tgt,\n","            is_training=False,\n","            drop_remainder=True,\n","            is_distributed=is_distributed)({'batch_size': batch_size})\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-ZTdh-uLL0P","colab_type":"code","outputId":"d330b1da-824a-4cd0-cbbe-09923074d617","executionInfo":{"status":"ok","timestamp":1573457676626,"user_tz":-330,"elapsed":1157,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["\n","bert_config = model_utils.transform_bert_to_texar_config(\n","            os.path.join(bert_pretrain_dir, 'bert_config.json'))\n","\n","\n","\n","tokenizer = tokenization.FullTokenizer(\n","        vocab_file=os.path.join(bert_pretrain_dir, 'vocab.txt'),\n","        do_lower_case=True)\n","\n","vocab_size = len(tokenizer.vocab)\n","\n","processor = CNNDailymail()\n","train_dataset = get_dataset(processor,tokenizer,\"/content\",max_seq_length_src,max_seq_length_tgt,64,'train',\"/content\")\n","eval_dataset = get_dataset(processor,tokenizer,\"/content\",max_seq_length_src,max_seq_length_tgt,64,'eval',\"/content\")\n","test_dataset = get_dataset(processor,tokenizer,\"/content\",max_seq_length_src,max_seq_length_tgt,64,'test',\"/content\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:distributed mode is not enabled.\n","{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(40,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(40,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(40,) dtype=int64>}\n","dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n","{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(40,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(40,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(40,) dtype=int64>}\n","dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n","{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(40,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(40,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(40,) dtype=int64>}\n","dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yHZUuKrLeII","colab_type":"code","outputId":"300c09e1-f69c-4ef2-c7e8-9ea54b1768c9","executionInfo":{"status":"ok","timestamp":1573457539782,"user_tz":-330,"elapsed":1018,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["src_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n","src_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n","tgt_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n","tgt_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n","\n","batch_size = tf.shape(src_input_ids)[0]\n","\n","src_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(src_input_ids, 0)),\n","                             axis=1)\n","tgt_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(tgt_input_ids, 0)),\n","                             axis=1)\n","\n","labels = tf.placeholder(tf.int64, shape=(None, None))\n","is_target = tf.to_float(tf.not_equal(labels, 0))\n","\n","\n","global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n","learning_rate = tf.placeholder(tf.float64, shape=(), name='lr')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-7f3b3cb91704>:14: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yov0SDbRN33j","colab_type":"code","colab":{}},"source":["iterator = tx.data.FeedableDataIterator({\n","        'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset})\n","\n","batch = iterator.get_next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G67G4zIVN9Ai","colab_type":"code","outputId":"8d635135-3bd0-42d1-9311-3b23a108e1f8","executionInfo":{"status":"ok","timestamp":1573457546737,"user_tz":-330,"elapsed":4095,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":666}},"source":["\n","print(\"Intializing the Bert Encoder Graph\")\n","with tf.variable_scope('bert',reuse=tf.AUTO_REUSE):\n","        embedder = tx.modules.WordEmbedder(\n","            vocab_size=bert_config.vocab_size,\n","            hparams=bert_config.embed)\n","        word_embeds = embedder(src_input_ids)\n","\n","        # Creates segment embeddings for each type of tokens.\n","        segment_embedder = tx.modules.WordEmbedder(\n","            vocab_size=bert_config.type_vocab_size,\n","            hparams=bert_config.segment_embed)\n","        segment_embeds = segment_embedder(src_segment_ids)\n","\n","        input_embeds = word_embeds + segment_embeds\n","\n","        # The BERT model (a TransformerEncoder)\n","        encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder)\n","        encoder_output = encoder(input_embeds, src_input_length,mode=tf.estimator.ModeKeys.PREDICT)\n","        \n","        # Builds layers for downstream classification, which is also initialized\n","        # with BERT pre-trained checkpoint.\n","        with tf.variable_scope(\"pooler\"):\n","            # Uses the projection of the 1st-step hidden vector of BERT output\n","            # as the representation of the sentence\n","            bert_sent_hidden = tf.squeeze(encoder_output[:, 0:1, :], axis=1)\n","            bert_sent_output = tf.layers.dense(\n","                bert_sent_hidden, config_downstream.hidden_dim,\n","                activation=tf.tanh)\n","            output = tf.layers.dropout(\n","                bert_sent_output, rate=0.1, training=tx.global_mode_train())\n","\n","\n","print(\"loading the bert pretrained weights\")\n","# Loads pretrained BERT model parameters\n","init_checkpoint = os.path.join(bert_pretrain_dir, 'bert_model.ckpt')\n","#init_checkpoint = \"gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\"\n","model_utils.init_bert_checkpoint(init_checkpoint)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Intializing the Bert Encoder Graph\n","WARNING:tensorflow:From texar_repo/texar/module_base.py:72: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/modules/embedders/embedder_utils.py:205: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/context.py:52: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/context.py:55: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/lib/python3.6/pydoc.py:1595: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:597: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/modules/encoders/transformer_encoders.py:340: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From texar_repo/texar/module_base.py:129: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/module_base.py:130: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/modules/networks/network_base.py:122: The name tf.layers.Dropout is deprecated. Please use tf.compat.v1.layers.Dropout instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/modules/networks/network_base.py:123: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/core/layers.py:1201: The name tf.erf is deprecated. Please use tf.math.erf instead.\n","\n","WARNING:tensorflow:From <ipython-input-8-79b8eee57fd2>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","loading the bert pretrained weights\n","WARNING:tensorflow:From /content/texar_repo/examples/bert/utils/model_utils.py:174: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/texar_repo/examples/bert/utils/model_utils.py:179: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xt4W17uHODMs","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4a26Q89aOHKa","colab_type":"code","outputId":"c676dd75-8cad-44c5-f105-3d9d00356239","executionInfo":{"status":"ok","timestamp":1573457560808,"user_tz":-330,"elapsed":8583,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["#decoder part and mle losss\n","tgt_embedding = tf.concat(\n","    [tf.zeros(shape=[1, embedder.dim]), embedder.embedding[1:, :]], axis=0)\n","\n","decoder = tx.modules.TransformerDecoder(embedding=tgt_embedding,\n","                             hparams=dcoder_config)\n","# For training\n","outputs = decoder(\n","    memory=encoder_output,\n","    memory_sequence_length=src_input_length,\n","    inputs=embedder(tgt_input_ids),\n","    sequence_length=tgt_input_length,\n","    decoding_strategy='train_greedy',\n","    mode=tf.estimator.ModeKeys.TRAIN\n",")\n","\n","mle_loss = transformer_utils.smoothing_cross_entropy(\n","        outputs.logits, labels, vocab_size, loss_label_confidence)\n","mle_loss = tf.reduce_sum(mle_loss * is_target) / tf.reduce_sum(is_target)\n","\n","tvars =tf.trainable_variables()\n","\n","non_bert_vars = [var for var in tvars if 'bert' not in var.name]\n","\n","\n","\n","\n","\n","train_op = tx.core.get_train_op(\n","        mle_loss,\n","        learning_rate=learning_rate,\n","        variables= non_bert_vars,\n","        global_step=global_step,\n","        hparams=opt)\n","\n","tf.summary.scalar('lr', learning_rate)\n","tf.summary.scalar('mle_loss', mle_loss)\n","summary_merged = tf.summary.merge_all()\n","\n","saver = tf.train.Saver(max_to_keep=5)\n","best_results = {'score': 0, 'epoch': -1}\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From texar_repo/texar/modules/decoders/transformer_decoders.py:88: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/utils/transformer_attentions.py:106: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n","\n","WARNING:tensorflow:From texar_repo/texar/modules/embedders/position_embedders.py:295: The name tf.mod is deprecated. Please use tf.math.mod instead.\n","\n","WARNING:tensorflow:From /usr/lib/python3.6/pydoc.py:1595: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-r7C_T58O-zB","colab_type":"code","outputId":"f7f9e882-b7be-4d05-c3fc-6795edf2e7d4","executionInfo":{"status":"ok","timestamp":1573457563742,"user_tz":-330,"elapsed":8992,"user":{"displayName":"Santhosh Kolloju","photoUrl":"","userId":"04379660425763606530"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["beam_width=1\n","start_tokens = tf.fill([tx.utils.get_batch_size(src_input_ids)],\n","                       bos_token_id)\n","predictions = decoder(\n","    memory=encoder_output,\n","    memory_sequence_length=src_input_length,\n","    decoding_strategy='infer_greedy',\n","    beam_width=beam_width,\n","    alpha=alpha,\n","    start_tokens=start_tokens,\n","    end_token=eos_token_id,\n","    max_decoding_length=30,\n","    mode=tf.estimator.ModeKeys.PREDICT\n",")\n","if beam_width <= 1:\n","    inferred_ids = predictions[0].sample_id\n","else:\n","    # Uses the best sample by beam search\n","    inferred_ids = predictions['sample_id'][:, :, 0]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From texar_repo/texar/modules/decoders/transformer_decoders.py:636: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1PzYmawMPCbp","colab_type":"code","colab":{}},"source":["def _train_epoch(sess, epoch, step, smry_writer):\n","        \n","            \n","        fetches = {\n","            'step': global_step,\n","            'train_op': train_op,\n","            'smry': summary_merged,\n","            'loss': mle_loss,\n","        }\n","\n","        while True:\n","            try:\n","              feed_dict = {\n","                iterator.handle: iterator.get_handle(sess, 'train'),\n","                tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n","              }\n","              op = sess.run([batch],feed_dict)\n","              feed_dict = {\n","                   src_input_ids:op[0]['src_input_ids'],\n","                   src_segment_ids : op[0]['src_segment_ids'],\n","                   tgt_input_ids:op[0]['tgt_input_ids'],\n","\n","                   labels:op[0]['tgt_labels'],\n","                   learning_rate: utils.get_lr(step, lr),\n","                   tx.global_mode(): tf.estimator.ModeKeys.TRAIN\n","                }\n","\n","\n","              fetches_ = sess.run(fetches, feed_dict=feed_dict)\n","              step, loss = fetches_['step'], fetches_['loss']\n","              if step % 100 ==0:\n","                  logger.info('step: %d, loss: %.4f', step, loss)\n","                  print('step: %d, loss: %.4f' % (step, loss))\n","                  smry_writer.add_summary(fetches_['smry'], global_step=step)\n","\n","              if step and step % 1000 == 0:\n","                  model_path = \"gs://answer2qa/checkpoint_\"+str(step)+\".ckpt\"\n","                  logger.info('saving model to %s', model_path)\n","                  print('saving model to %s' % model_path)\n","                  saver.save(sess, model_path)\n","              if step and step % eval_steps == 0:\n","                  _eval_epoch(sess, epoch, mode='eval')\n","            except tf.errors.OutOfRangeError:\n","                break\n","\n","        return step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmvz6EJ5YX16","colab_type":"code","colab":{}},"source":["from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgARl7uSPIkq","colab_type":"code","outputId":"f398363a-12b1-41c2-9e01-6769cb2223ad","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model_dir = \"gs://answer2qa/\"\n","logging_file= \"logging.txt\"\n","logger = utils.get_logger(logging_file)\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.local_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","\n","    smry_writer = tf.summary.FileWriter(\"./\", graph=sess.graph)\n","\n","    if run_mode == 'train_and_evaluate':\n","        logger.info('Begin running with train_and_evaluate mode')\n","\n","        if tf.train.latest_checkpoint(model_dir) is not None:\n","            logger.info('Restore latest checkpoint in %s' % model_dir)\n","            saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n","        \n","        iterator.initialize_dataset(sess)\n","\n","        step = 0\n","        for epoch in range(max_train_epoch):\n","          iterator.restart_dataset(sess, 'train')\n","          step = _train_epoch(sess, epoch, step, smry_writer)\n","\n","    elif run_mode == 'test':\n","        logger.info('Begin running with test mode')\n","\n","        logger.info('Restore latest checkpoint in %s' % model_dir)\n","        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n","\n","        _eval_epoch(sess, 0, mode='test')\n","\n","    else:\n","        raise ValueError('Unknown mode: {}'.format(run_mode))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["step: 0, loss: 12.5727\n","step: 100, loss: 8.6917\n","step: 200, loss: 7.2478\n","step: 300, loss: 6.9473\n","step: 400, loss: 6.6794\n","step: 500, loss: 6.4756\n","step: 600, loss: 6.1462\n","step: 700, loss: 6.3209\n","step: 800, loss: 5.9999\n","step: 900, loss: 6.0397\n","step: 1000, loss: 5.9714\n","saving model to gs://answer2qa/checkpoint_1000.ckpt\n","step: 1100, loss: 5.7386\n","step: 1200, loss: 5.7992\n","step: 1300, loss: 5.8017\n","step: 1400, loss: 5.7387\n","step: 1500, loss: 5.8233\n","step: 1600, loss: 5.5906\n","step: 1700, loss: 5.5528\n","step: 1800, loss: 5.7216\n","step: 1900, loss: 5.6969\n","step: 2000, loss: 5.7520\n","saving model to gs://answer2qa/checkpoint_2000.ckpt\n","step: 2100, loss: 5.5438\n","step: 2200, loss: 5.4544\n","step: 2300, loss: 5.4859\n","step: 2400, loss: 5.0763\n","step: 2500, loss: 5.1476\n","step: 2600, loss: 4.8347\n","step: 2700, loss: 5.0251\n","step: 2800, loss: 5.0671\n","step: 2900, loss: 5.0653\n","step: 3000, loss: 4.7156\n","saving model to gs://answer2qa/checkpoint_3000.ckpt\n","step: 3100, loss: 4.9589\n","step: 3200, loss: 4.8962\n","step: 3300, loss: 4.6228\n","step: 3400, loss: 4.6182\n","step: 3500, loss: 4.6735\n","step: 3600, loss: 4.7842\n","step: 3700, loss: 4.6702\n","step: 3800, loss: 4.7643\n","step: 3900, loss: 4.9590\n","step: 4000, loss: 4.9305\n","saving model to gs://answer2qa/checkpoint_4000.ckpt\n","step: 4100, loss: 4.7463\n","step: 4200, loss: 5.0184\n","step: 4300, loss: 4.9187\n","step: 4400, loss: 4.7790\n","step: 4500, loss: 4.9715\n","step: 4600, loss: 5.0151\n","step: 4700, loss: 4.8436\n","step: 4800, loss: 4.7455\n","step: 4900, loss: 4.7513\n","step: 5000, loss: 4.7998\n","saving model to gs://answer2qa/checkpoint_5000.ckpt\n","step: 5100, loss: 4.9549\n","step: 5200, loss: 4.5119\n","step: 5300, loss: 4.7766\n","step: 5400, loss: 4.7017\n","step: 5500, loss: 4.7646\n","step: 5600, loss: 4.7380\n","step: 5700, loss: 4.6417\n","step: 5800, loss: 4.9438\n","step: 5900, loss: 4.7005\n","step: 6000, loss: 4.5871\n","saving model to gs://answer2qa/checkpoint_6000.ckpt\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","step: 6100, loss: 4.8339\n","step: 6200, loss: 4.7544\n","step: 6300, loss: 4.6894\n","step: 6400, loss: 4.6351\n","step: 6500, loss: 4.6104\n","step: 6600, loss: 4.6652\n","step: 6700, loss: 4.4201\n","step: 6800, loss: 4.4468\n","step: 6900, loss: 4.5466\n","step: 7000, loss: 4.5737\n","saving model to gs://answer2qa/checkpoint_7000.ckpt\n","step: 7100, loss: 4.4290\n","step: 7200, loss: 4.7641\n","step: 7300, loss: 4.5063\n","step: 7400, loss: 4.5475\n","step: 7500, loss: 4.4028\n","step: 7600, loss: 4.5476\n","step: 7700, loss: 4.5601\n","step: 7800, loss: 4.7207\n","step: 7900, loss: 5.4242\n","step: 8000, loss: 5.3478\n","saving model to gs://answer2qa/checkpoint_8000.ckpt\n","step: 8100, loss: 5.2826\n","step: 8200, loss: 5.2067\n","step: 8300, loss: 5.3582\n","step: 8400, loss: 5.1883\n","step: 8500, loss: 5.2278\n","step: 8600, loss: 5.0583\n","step: 8700, loss: 5.1430\n","step: 8800, loss: 4.9325\n","step: 8900, loss: 5.1956\n","step: 9000, loss: 5.1378\n","saving model to gs://answer2qa/checkpoint_9000.ckpt\n","step: 9100, loss: 5.1153\n","step: 9200, loss: 5.2341\n","step: 9300, loss: 4.9684\n","step: 9400, loss: 5.0228\n","step: 9500, loss: 5.1874\n","step: 9600, loss: 5.1439\n","step: 9700, loss: 5.1417\n","step: 9800, loss: 5.1910\n","step: 9900, loss: 5.3559\n","step: 10000, loss: 5.4706\n","saving model to gs://answer2qa/checkpoint_10000.ckpt\n","step: 10100, loss: 4.8602\n","step: 10200, loss: 4.5386\n","step: 10300, loss: 4.6441\n","step: 10400, loss: 4.2976\n","step: 10500, loss: 4.4251\n","step: 10600, loss: 4.4350\n","step: 10700, loss: 4.3185\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"92C6ZGx3QZ9o","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}